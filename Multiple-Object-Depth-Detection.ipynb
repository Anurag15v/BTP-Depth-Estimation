{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (0.10.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from mediapipe) (3.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from mediapipe) (1.25.2)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from mediapipe) (4.8.0.76)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from matplotlib->mediapipe) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from matplotlib->mediapipe) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from matplotlib->mediapipe) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from matplotlib->mediapipe) (10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from matplotlib->mediapipe) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (1.11.2)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from scipy) (1.25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (0.9.7)\n",
      "Requirement already satisfied: torch>=1.7 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from timm) (2.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from timm) (0.15.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from timm) (0.17.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from timm) (0.3.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch>=1.7->timm) (3.12.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch>=1.7->timm) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch>=1.7->timm) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch>=1.7->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from huggingface-hub->timm) (2023.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from huggingface-hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from huggingface-hub->timm) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from huggingface-hub->timm) (23.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torchvision->timm) (1.25.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from torchvision->timm) (10.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from requests->huggingface-hub->timm) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from requests->huggingface-hub->timm) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from requests->huggingface-hub->timm) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\anura\\onedrive\\desktop\\btp_code\\env\\lib\\site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anura\\OneDrive\\Desktop\\BTP_Code\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import timm\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\anura/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\anura/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MidasNet_small(\n",
       "  (pretrained): Module(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2dSameExport(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "      (3): Sequential(\n",
       "        (0): DepthwiseSeparableConv(\n",
       "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pw): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2dSameExport(144, 144, kernel_size=(3, 3), stride=(2, 2), groups=144, bias=False)\n",
       "          (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2dSameExport(192, 192, kernel_size=(5, 5), stride=(2, 2), groups=192, bias=False)\n",
       "          (bn2): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2dSameExport(288, 288, kernel_size=(3, 3), stride=(2, 2), groups=288, bias=False)\n",
       "          (bn2): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (bn2): BatchNorm2d(576, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(136, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2dSameExport(816, 816, kernel_size=(5, 5), stride=(2, 2), groups=816, bias=False)\n",
       "          (bn2): BatchNorm2d(816, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (5): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(232, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): InvertedResidual(\n",
       "          (conv_pw): Conv2d(232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act1): ReLU6(inplace=True)\n",
       "          (conv_dw): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
       "          (bn2): BatchNorm2d(1392, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (act2): ReLU6(inplace=True)\n",
       "          (se): Identity()\n",
       "          (conv_pwl): Conv2d(1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (scratch): Module(\n",
       "    (layer1_rn): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer2_rn): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer3_rn): Conv2d(136, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer4_rn): Conv2d(384, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (activation): ReLU()\n",
       "    (refinenet4): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet3): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet2): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (refinenet1): FeatureFusionBlock_custom(\n",
       "      (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (resConfUnit1): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (resConfUnit2): ResidualConvUnit_custom(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (activation): ReLU()\n",
       "        (skip_add): FloatFunctional(\n",
       "          (activation_post_process): Identity()\n",
       "        )\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "    )\n",
       "    (output_conv): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Interpolate()\n",
       "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Downloading the model from TorchHub.\n",
    "midas = torch.hub.load('intel-isl/MiDaS','MiDaS_small')\n",
    "midas.to('cpu')\n",
    "midas.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\anura/.cache\\torch\\hub\\intel-isl_MiDaS_master\n"
     ]
    }
   ],
   "source": [
    "# Load MiDaS transforms\n",
    "transforms = torch.hub.load('intel-isl/MiDaS', 'transforms')\n",
    "transform = transforms.small_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'cv2.TrackerKCF' object has no attribute 'get_position'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\anura\\OneDrive\\Desktop\\BTP_Code\\Multiple-Object-Depth-Detection.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anura/OneDrive/Desktop/BTP_Code/Multiple-Object-Depth-Detection.ipynb#X10sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m success \u001b[39m=\u001b[39m tracker_info[\u001b[39m\"\u001b[39m\u001b[39mtracker\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(frame)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anura/OneDrive/Desktop/BTP_Code/Multiple-Object-Depth-Detection.ipynb#X10sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/anura/OneDrive/Desktop/BTP_Code/Multiple-Object-Depth-Detection.ipynb#X10sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     tracker_box \u001b[39m=\u001b[39m tracker_info[\u001b[39m\"\u001b[39;49m\u001b[39mtracker\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mget_position()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anura/OneDrive/Desktop/BTP_Code/Multiple-Object-Depth-Detection.ipynb#X10sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     tx, ty, tw, th \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(tracker_box[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/anura/OneDrive/Desktop/BTP_Code/Multiple-Object-Depth-Detection.ipynb#X10sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     color \u001b[39m=\u001b[39m tracker_info[\u001b[39m\"\u001b[39m\u001b[39mcolor\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'cv2.TrackerKCF' object has no attribute 'get_position'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "previous_depth = 0.0\n",
    "depth_scale = 1.0\n",
    "\n",
    "# Define depth to distance\n",
    "def depth_to_distance(depth_value, depth_scale):\n",
    "    return 1.0 / (depth_value * depth_scale)\n",
    "\n",
    "# Applying exponential moving average filter\n",
    "def apply_ema_filter(current_depth):\n",
    "    global previous_depth\n",
    "    filtered_depth = alpha * current_depth + (1 - alpha) * previous_depth\n",
    "    previous_depth = filtered_depth  # Update the previous depth value\n",
    "    return filtered_depth\n",
    "\n",
    "# Initialize YOLO object detection\n",
    "yolo = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as file:\n",
    "    classes = [line.strip() for line in file.readlines()]\n",
    "\n",
    "layer_names = yolo.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo.getUnconnectedOutLayers()]\n",
    "\n",
    "# Create a dictionary to store object trackers with unique colors\n",
    "object_trackers = {}\n",
    "next_object_id = 1\n",
    "\n",
    "# Create a dictionary to store the color assigned to each person\n",
    "person_colors = defaultdict(lambda: (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255)))\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Detect objects using YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    yolo.setInput(blob)\n",
    "    outputs = yolo.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                w = int(detection[2] * frame.shape[1])\n",
    "                h = int(detection[3] * frame.shape[0])\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Initialize a list to store object IDs to be removed\n",
    "    objects_to_remove = []\n",
    "\n",
    "    for object_id, tracker_info in object_trackers.items():\n",
    "        success = tracker_info[\"tracker\"].update(frame)\n",
    "        if success:\n",
    "            tracker_box = tracker_info[\"tracker\"].get_position()\n",
    "            tx, ty, tw, th = [int(tracker_box[i]) for i in range(4)]\n",
    "            color = tracker_info[\"color\"]\n",
    "            label = tracker_info[\"label\"]\n",
    "        else:\n",
    "            # Handle the case where tracking fails (e.g., object disappeared)\n",
    "            # You can choose to remove the tracker or take other actions here\n",
    "            objects_to_remove.append(object_id)\n",
    "            continue\n",
    "\n",
    "        object_found = False\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                if x >= tx and y >= ty and x + w <= tx + tw and y + h <= ty + th:\n",
    "                    object_found = True\n",
    "                    break\n",
    "\n",
    "        if not object_found:\n",
    "            objects_to_remove.append(object_id)\n",
    "        else:\n",
    "            # Update the tracker with the new position\n",
    "            object_trackers[object_id][\"tracker\"] = tracker_info[\"tracker\"]\n",
    "\n",
    "            object_frame = frame[y:y + h, x:x + w]  # Extract the object frame\n",
    "\n",
    "            # Check if the object frame is not empty (width and height are not zero)\n",
    "            if object_frame.shape[0] > 0 and object_frame.shape[1] > 0:\n",
    "                # Apply MiDaS depth estimation to the object frame\n",
    "                object_imgbatch = transform(object_frame).to('cpu')\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prediction = midas(object_imgbatch)\n",
    "                    prediction = torch.nn.functional.interpolate(\n",
    "                        prediction.unsqueeze(1),\n",
    "                        size=object_frame.shape[:2],\n",
    "                        mode='bicubic',\n",
    "                        align_corners=False\n",
    "                    ).squeeze()\n",
    "\n",
    "                object_output = prediction.cpu().numpy()\n",
    "                object_output_norm = cv2.normalize(object_output, None, 0, 1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "                # Calculate depth for the object\n",
    "                h_obj, w_obj = object_output_norm.shape\n",
    "                x_grid_obj = np.arange(w_obj)\n",
    "                y_grid_obj = np.arange(h_obj)\n",
    "\n",
    "                spline_obj = RectBivariateSpline(y_grid_obj, x_grid_obj, object_output_norm)\n",
    "                depth_obj_filt = spline_obj(h_obj / 2, w_obj / 2)\n",
    "                depth_obj_midas = depth_to_distance(depth_obj_filt, depth_scale)\n",
    "                depth_obj_filt = (apply_ema_filter(depth_obj_midas) / 10)[0][0]\n",
    "\n",
    "                # Display the object and its depth with depth at the top of the bounding box\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                text = f\"{label} - Depth: {depth_obj_filt:.2f} m\"\n",
    "                cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Remove objects that are no longer being tracked\n",
    "    for object_id in objects_to_remove:\n",
    "        del object_trackers[object_id]\n",
    "\n",
    "    # Detect and track new objects\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "\n",
    "            object_found = False\n",
    "            for object_id, tracker_info in object_trackers.items():\n",
    "                (success, tracker_box) = tracker_info[\"tracker\"].update(frame)\n",
    "                if success:\n",
    "                    tx, ty, tw, th = [int(tracker_box[i]) for i in range(4)]\n",
    "                    if x >= tx and y >= ty and x + w <= tx + tw and y + h <= ty + th:\n",
    "                        object_found = True\n",
    "                        break\n",
    "\n",
    "            if not object_found:\n",
    "                # Assign a unique color based on the label\n",
    "                color = person_colors[label]\n",
    "\n",
    "                # Create a tracker for the new object\n",
    "                tracker = cv2.TrackerKCF_create()\n",
    "                tracker.init(frame, (x, y, w, h))\n",
    "\n",
    "                # Store the tracker along with its color and label\n",
    "                object_trackers[next_object_id] = {\"tracker\": tracker, \"color\": color, \"label\": label}\n",
    "                next_object_id += 1\n",
    "\n",
    "    cv2.imshow('image', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue Of Global Previous Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "previous_depth = 0.0\n",
    "depth_scale = 1.0\n",
    "\n",
    "# Define depth to distance\n",
    "def depth_to_distance(depth_value, depth_scale):\n",
    "    return 1.0 / (depth_value * depth_scale)\n",
    "\n",
    "# Applying exponential moving average filter\n",
    "def apply_ema_filter(current_depth):\n",
    "    global previous_depth\n",
    "    filtered_depth = alpha * current_depth + (1 - alpha) * previous_depth\n",
    "    previous_depth = filtered_depth  # Update the previous depth value\n",
    "    return filtered_depth\n",
    "\n",
    "# Initialize YOLO object detection\n",
    "yolo = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as file:\n",
    "    classes = [line.strip() for line in file.readlines()]\n",
    "\n",
    "layer_names = yolo.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo.getUnconnectedOutLayers()]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Detect objects using YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    yolo.setInput(blob)\n",
    "    outputs = yolo.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                w = int(detection[2] * frame.shape[1])\n",
    "                h = int(detection[3] * frame.shape[0])\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "\n",
    "            object_frame = frame[y:y + h, x:x + w]  # Extract the object frame\n",
    "\n",
    "            # Check if the object frame is not empty (width and height are not zero)\n",
    "            if object_frame.shape[0] > 0 and object_frame.shape[1] > 0:\n",
    "                # Apply MiDaS depth estimation to the object frame\n",
    "                object_imgbatch = transform(object_frame).to('cpu')\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prediction = midas(object_imgbatch)\n",
    "                    prediction = torch.nn.functional.interpolate(\n",
    "                        prediction.unsqueeze(1),\n",
    "                        size=object_frame.shape[:2],\n",
    "                        mode='bicubic',\n",
    "                        align_corners=False\n",
    "                    ).squeeze()\n",
    "\n",
    "                object_output = prediction.cpu().numpy()\n",
    "                object_output_norm = cv2.normalize(object_output, None, 0, 1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "                # Calculate depth for the object\n",
    "                h_obj, w_obj = object_output_norm.shape\n",
    "                x_grid_obj = np.arange(w_obj)\n",
    "                y_grid_obj = np.arange(h_obj)\n",
    "\n",
    "                spline_obj = RectBivariateSpline(y_grid_obj, x_grid_obj, object_output_norm)\n",
    "                depth_obj_filt = spline_obj(h_obj / 2, w_obj / 2)\n",
    "                depth_obj_midas = depth_to_distance(depth_obj_filt, depth_scale)\n",
    "                depth_obj_filt = (apply_ema_filter(depth_obj_midas) / 10)[0][0]\n",
    "\n",
    "                # Display the object and its depth with depth at the top of the bounding box\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Depth: {depth_obj_filt:.2f} m\", (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('image', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "\n",
    "# Assuming you have the necessary imports and definitions for midas and transform\n",
    "\n",
    "alpha = 0.2\n",
    "depth_scale = 1.0\n",
    "previous_depths = {}\n",
    "\n",
    "# Define depth to distance\n",
    "def depth_to_distance(depth_value, depth_scale):\n",
    "    return 1.0 / (depth_value * depth_scale)\n",
    "\n",
    "# Applying exponential moving average filter\n",
    "def apply_ema_filter(current_depth, object_index):\n",
    "    global previous_depths\n",
    "    if object_index not in previous_depths:\n",
    "        previous_depths[object_index] = 0.0  # Initialize previous depth for this object\n",
    "\n",
    "    filtered_depth = alpha * current_depth + (1 - alpha) * previous_depths[object_index]\n",
    "    previous_depths[object_index] = filtered_depth  # Update the previous depth value for this object\n",
    "    return filtered_depth\n",
    "\n",
    "# Initialize YOLO object detection\n",
    "yolo = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as file:\n",
    "    classes = [line.strip() for line in file.readlines()]\n",
    "\n",
    "layer_names = yolo.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo.getUnconnectedOutLayers()]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Detect objects using YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    yolo.setInput(blob)\n",
    "    outputs = yolo.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                w = int(detection[2] * frame.shape[1])\n",
    "                h = int(detection[3] * frame.shape[0])\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "\n",
    "            object_frame = frame[y:y + h, x:x + w]  # Extract the object frame\n",
    "\n",
    "            # Check if the object frame is not empty (width and height are not zero)\n",
    "            if object_frame.shape[0] > 0 and object_frame.shape[1] > 0:\n",
    "                # Apply MiDaS depth estimation to the object frame\n",
    "                object_imgbatch = transform(object_frame).to('cpu')\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prediction = midas(object_imgbatch)\n",
    "                    prediction = torch.nn.functional.interpolate(\n",
    "                        prediction.unsqueeze(1),\n",
    "                        size=object_frame.shape[:2],\n",
    "                        mode='bicubic',\n",
    "                        align_corners=False\n",
    "                    ).squeeze()\n",
    "\n",
    "                object_output = prediction.cpu().numpy()\n",
    "                object_output_norm = cv2.normalize(object_output, None, 0, 1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "                # Calculate depth for the object\n",
    "                h_obj, w_obj = object_output_norm.shape\n",
    "                x_grid_obj = np.arange(w_obj)\n",
    "                y_grid_obj = np.arange(h_obj)\n",
    "\n",
    "                spline_obj = RectBivariateSpline(y_grid_obj, x_grid_obj, object_output_norm)\n",
    "                depth_obj_filt = spline_obj(h_obj / 2, w_obj / 2)\n",
    "                depth_obj_midas = depth_to_distance(depth_obj_filt, depth_scale)\n",
    "                depth_obj_filt = (apply_ema_filter(depth_obj_midas, i) / 10)[0][0]\n",
    "\n",
    "                # Display the object and its depth with depth at the top of the bounding box\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Depth: {depth_obj_filt:.2f} m\", (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('image', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaussian Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "\n",
    "# Assuming you have the necessary imports and definitions for midas and transform\n",
    "\n",
    "alpha = 0.2\n",
    "depth_scale = 1.0\n",
    "previous_depths = {}\n",
    "\n",
    "# Define depth to distance\n",
    "def depth_to_distance(depth_value, depth_scale):\n",
    "    return 1.0 / (depth_value * depth_scale)\n",
    "\n",
    "# Applying exponential moving average filter\n",
    "def apply_ema_filter(current_depth, object_index):\n",
    "    global previous_depths\n",
    "    if object_index not in previous_depths:\n",
    "        previous_depths[object_index] = 0.0  # Initialize previous depth for this object\n",
    "\n",
    "    filtered_depth = alpha * current_depth + (1 - alpha) * previous_depths[object_index]\n",
    "    previous_depths[object_index] = filtered_depth  # Update the previous depth value for this object\n",
    "    return filtered_depth\n",
    "\n",
    "# Initialize YOLO object detection\n",
    "yolo = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "classes = []\n",
    "\n",
    "with open(\"coco.names\", \"r\") as file:\n",
    "    classes = [line.strip() for line in file.readlines()]\n",
    "\n",
    "layer_names = yolo.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo.getUnconnectedOutLayers()]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Detect objects using YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    yolo.setInput(blob)\n",
    "    outputs = yolo.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                w = int(detection[2] * frame.shape[1])\n",
    "                h = int(detection[3] * frame.shape[0])\n",
    "\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        if i in indexes:\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "\n",
    "            object_frame = frame[y:y + h, x:x + w]  # Extract the object frame\n",
    "\n",
    "            # Check if the object frame is not empty (width and height are not zero)\n",
    "            if object_frame.shape[0] > 0 and object_frame.shape[1] > 0:\n",
    "                # Apply MiDaS depth estimation to the object frame\n",
    "                object_imgbatch = transform(object_frame).to('cpu')\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prediction = midas(object_imgbatch)\n",
    "                    prediction = torch.nn.functional.interpolate(\n",
    "                        prediction.unsqueeze(1),\n",
    "                        size=object_frame.shape[:2],\n",
    "                        mode='bicubic',\n",
    "                        align_corners=False\n",
    "                    ).squeeze()\n",
    "\n",
    "                object_output = prediction.cpu().numpy()\n",
    "                object_output_norm = cv2.normalize(object_output, None, 0, 1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "                # Apply Gaussian blur to the depth map before filtering\n",
    "                object_output_norm_blur = cv2.GaussianBlur(object_output_norm, (5, 5), 0)\n",
    "\n",
    "                # Calculate depth for the object\n",
    "                h_obj, w_obj = object_output_norm_blur.shape\n",
    "                x_grid_obj = np.arange(w_obj)\n",
    "                y_grid_obj = np.arange(h_obj)\n",
    "\n",
    "                spline_obj = RectBivariateSpline(y_grid_obj, x_grid_obj, object_output_norm_blur)\n",
    "                depth_obj_filt = spline_obj(h_obj / 2, w_obj / 2)\n",
    "                depth_obj_midas = depth_to_distance(depth_obj_filt, depth_scale)\n",
    "                depth_obj_filt = (apply_ema_filter(depth_obj_midas, i) / 10)[0][0]\n",
    "\n",
    "                # Display the object and its depth with depth at the top of the bounding box\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Depth: {depth_obj_filt:.2f} m\", (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('image', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
